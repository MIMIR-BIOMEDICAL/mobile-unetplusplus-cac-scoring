{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Resources\r\n",
    "- Huang, H., Lin, L., Tong, R., Hu, H., Zhang, Q., Iwamoto, Y., Han, X., Chen, Y.W. and Wu, J., 2020. UNet 3+: A Full-Scale Connected UNet for Medical Image Segmentation. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 1055-1059). IEEE.\r\n",
    "- [keras-unet-collection](https://github.com/yingkaisha/keras-unet-collection)\r\n",
    "- [Volume Segmentation using UNet](https://github.com/madsendennis/notebooks/tree/master/volume_segmentation_with_unet)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Activation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "\r\n",
    "from tensorflow import math\r\n",
    "from tensorflow.keras.layers import Layer\r\n",
    "import tensorflow.keras.backend as K\r\n",
    "\r\n",
    "\r\n",
    "def gelu_(X):\r\n",
    "\r\n",
    "    return 0.5*X*(1.0 + math.tanh(0.7978845608028654*(X + 0.044715*math.pow(X, 3))))\r\n",
    "\r\n",
    "def snake_(X, beta):\r\n",
    "\r\n",
    "    return X + (1/beta)*math.square(math.sin(beta*X))\r\n",
    "\r\n",
    "\r\n",
    "class GELU(Layer):\r\n",
    "    '''\r\n",
    "    Gaussian Error Linear Unit (GELU), an alternative of ReLU\r\n",
    "    \r\n",
    "    Y = GELU()(X)\r\n",
    "    \r\n",
    "    ----------\r\n",
    "    Hendrycks, D. and Gimpel, K., 2016. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415.\r\n",
    "    \r\n",
    "    Usage: use it as a tf.keras.Layer\r\n",
    "    \r\n",
    "    \r\n",
    "    '''\r\n",
    "    def __init__(self, trainable=False, **kwargs):\r\n",
    "        super(GELU, self).__init__(**kwargs)\r\n",
    "        self.supports_masking = True\r\n",
    "        self.trainable = trainable\r\n",
    "\r\n",
    "    def build(self, input_shape):\r\n",
    "        super(GELU, self).build(input_shape)\r\n",
    "\r\n",
    "    def call(self, inputs, mask=None):\r\n",
    "        return gelu_(inputs)\r\n",
    "\r\n",
    "    def get_config(self):\r\n",
    "        config = {'trainable': self.trainable}\r\n",
    "        base_config = super(GELU, self).get_config()\r\n",
    "        return dict(list(base_config.items()) + list(config.items()))\r\n",
    "    def compute_output_shape(self, input_shape):\r\n",
    "        return input_shape\r\n",
    "\r\n",
    "    \r\n",
    "class Snake(Layer):\r\n",
    "    '''\r\n",
    "    Snake activation function $X + (1/b)*sin^2(b*X)$. Proposed to learn periodic targets.\r\n",
    "    \r\n",
    "    Y = Snake(beta=0.5, trainable=False)(X)\r\n",
    "    \r\n",
    "    ----------\r\n",
    "    Ziyin, L., Hartwig, T. and Ueda, M., 2020. Neural networks fail to learn periodic functions \r\n",
    "    and how to fix it. arXiv preprint arXiv:2006.08195.\r\n",
    "    \r\n",
    "    '''\r\n",
    "    def __init__(self, beta=0.5, trainable=False, **kwargs):\r\n",
    "        super(Snake, self).__init__(**kwargs)\r\n",
    "        self.supports_masking = True\r\n",
    "        self.beta = beta\r\n",
    "        self.trainable = trainable\r\n",
    "\r\n",
    "    def build(self, input_shape):\r\n",
    "        self.beta_factor = K.variable(self.beta, dtype=K.floatx(), name='beta_factor')\r\n",
    "        if self.trainable:\r\n",
    "            self._trainable_weights.append(self.beta_factor)\r\n",
    "\r\n",
    "        super(Snake, self).build(input_shape)\r\n",
    "\r\n",
    "    def call(self, inputs, mask=None):\r\n",
    "        return snake_(inputs, self.beta_factor)\r\n",
    "\r\n",
    "    def get_config(self):\r\n",
    "        config = {'beta': self.get_weights()[0] if self.trainable else self.beta, 'trainable': self.trainable}\r\n",
    "        base_config = super(Snake, self).get_config()\r\n",
    "        return dict(list(base_config.items()) + list(config.items()))\r\n",
    "\r\n",
    "    def compute_output_shape(self, input_shape):\r\n",
    "        return input_shape\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Layer Utils"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from tensorflow import expand_dims\r\n",
    "from tensorflow.compat.v1 import image\r\n",
    "from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D, UpSampling2D, Conv2DTranspose, GlobalAveragePooling2D\r\n",
    "from tensorflow.keras.layers import Conv2D, DepthwiseConv2D, Lambda\r\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, concatenate, multiply, add\r\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU, PReLU, ELU, Softmax\r\n",
    "\r\n",
    "def decode_layer(X, channel, pool_size, unpool, kernel_size=3, \r\n",
    "                 activation='ReLU', batch_norm=False, name='decode'):\r\n",
    "    '''\r\n",
    "    An overall decode layer, based on either upsampling or trans conv.\r\n",
    "    \r\n",
    "    decode_layer(X, channel, pool_size, unpool, kernel_size=3,\r\n",
    "                 activation='ReLU', batch_norm=False, name='decode')\r\n",
    "    \r\n",
    "    Input\r\n",
    "    ----------\r\n",
    "        X: input tensor.\r\n",
    "        pool_size: the decoding factor.\r\n",
    "        channel: (for trans conv only) number of convolution filters.\r\n",
    "        unpool: True or 'bilinear' for Upsampling2D with bilinear interpolation.\r\n",
    "                'nearest' for Upsampling2D with nearest interpolation.\r\n",
    "                False for Conv2DTranspose + batch norm + activation.           \r\n",
    "        kernel_size: size of convolution kernels. \r\n",
    "                     If kernel_size='auto', then it equals to the `pool_size`.\r\n",
    "        activation: one of the `tensorflow.keras.layers` interface, e.g., ReLU.\r\n",
    "        batch_norm: True for batch normalization, False otherwise.\r\n",
    "        name: prefix of the created keras layers.\r\n",
    "        \r\n",
    "    Output\r\n",
    "    ----------\r\n",
    "        X: output tensor.\r\n",
    "    \r\n",
    "    * The defaut: `kernel_size=3`, is suitable for `pool_size=2`.\r\n",
    "    \r\n",
    "    '''\r\n",
    "    # parsers\r\n",
    "    if unpool is False:\r\n",
    "        # trans conv configurations\r\n",
    "        bias_flag = not batch_norm\r\n",
    "    \r\n",
    "    elif unpool == 'nearest':\r\n",
    "        # upsample2d configurations\r\n",
    "        unpool = True\r\n",
    "        interp = 'nearest'\r\n",
    "    \r\n",
    "    elif (unpool is True) or (unpool == 'bilinear'):\r\n",
    "        # upsample2d configurations\r\n",
    "        unpool = True\r\n",
    "        interp = 'bilinear'\r\n",
    "    \r\n",
    "    else:\r\n",
    "        raise ValueError('Invalid unpool keyword')\r\n",
    "        \r\n",
    "    if unpool:\r\n",
    "        X = UpSampling2D(size=(pool_size, pool_size), interpolation=interp, name='{}_unpool'.format(name))(X)\r\n",
    "    else:\r\n",
    "        if kernel_size == 'auto':\r\n",
    "            kernel_size = pool_size\r\n",
    "            \r\n",
    "        X = Conv2DTranspose(channel, kernel_size, strides=(pool_size, pool_size), \r\n",
    "                            padding='same', name='{}_trans_conv'.format(name))(X)\r\n",
    "        \r\n",
    "        # batch normalization\r\n",
    "        if batch_norm:\r\n",
    "            X = BatchNormalization(axis=3, name='{}_bn'.format(name))(X)\r\n",
    "            \r\n",
    "        # activation\r\n",
    "        if activation is not None:\r\n",
    "            activation_func = eval(activation)\r\n",
    "            X = activation_func(name='{}_activation'.format(name))(X)\r\n",
    "        \r\n",
    "    return X\r\n",
    "\r\n",
    "def encode_layer(X, channel, pool_size, pool, kernel_size='auto', \r\n",
    "                 activation='ReLU', batch_norm=False, name='encode'):\r\n",
    "    '''\r\n",
    "    An overall encode layer, based on one of the:\r\n",
    "    (1) max-pooling, (2) average-pooling, (3) strided conv2d.\r\n",
    "    \r\n",
    "    encode_layer(X, channel, pool_size, pool, kernel_size='auto', \r\n",
    "                 activation='ReLU', batch_norm=False, name='encode')\r\n",
    "    \r\n",
    "    Input\r\n",
    "    ----------\r\n",
    "        X: input tensor.\r\n",
    "        pool_size: the encoding factor.\r\n",
    "        channel: (for strided conv only) number of convolution filters.\r\n",
    "        pool: True or 'max' for MaxPooling2D.\r\n",
    "              'ave' for AveragePooling2D.\r\n",
    "              False for strided conv + batch norm + activation.\r\n",
    "        kernel_size: size of convolution kernels. \r\n",
    "                     If kernel_size='auto', then it equals to the `pool_size`.\r\n",
    "        activation: one of the `tensorflow.keras.layers` interface, e.g., ReLU.\r\n",
    "        batch_norm: True for batch normalization, False otherwise.\r\n",
    "        name: prefix of the created keras layers.\r\n",
    "        \r\n",
    "    Output\r\n",
    "    ----------\r\n",
    "        X: output tensor.\r\n",
    "        \r\n",
    "    '''\r\n",
    "    # parsers\r\n",
    "    if (pool in [False, True, 'max', 'ave']) is not True:\r\n",
    "        raise ValueError('Invalid pool keyword')\r\n",
    "        \r\n",
    "    # maxpooling2d as default\r\n",
    "    if pool is True:\r\n",
    "        pool = 'max'\r\n",
    "        \r\n",
    "    elif pool is False:\r\n",
    "        # stride conv configurations\r\n",
    "        bias_flag = not batch_norm\r\n",
    "    \r\n",
    "    if pool == 'max':\r\n",
    "        X = MaxPooling2D(pool_size=(pool_size, pool_size), name='{}_maxpool'.format(name))(X)\r\n",
    "        \r\n",
    "    elif pool == 'ave':\r\n",
    "        X = AveragePooling2D(pool_size=(pool_size, pool_size), name='{}_avepool'.format(name))(X)\r\n",
    "        \r\n",
    "    else:\r\n",
    "        if kernel_size == 'auto':\r\n",
    "            kernel_size = pool_size\r\n",
    "        \r\n",
    "        # linear convolution with strides\r\n",
    "        X = Conv2D(channel, kernel_size, strides=(pool_size, pool_size), \r\n",
    "                   padding='valid', use_bias=bias_flag, name='{}_stride_conv'.format(name))(X)\r\n",
    "        \r\n",
    "        # batch normalization\r\n",
    "        if batch_norm:\r\n",
    "            X = BatchNormalization(axis=3, name='{}_bn'.format(name))(X)\r\n",
    "            \r\n",
    "        # activation\r\n",
    "        if activation is not None:\r\n",
    "            activation_func = eval(activation)\r\n",
    "            X = activation_func(name='{}_activation'.format(name))(X)\r\n",
    "            \r\n",
    "    return X\r\n",
    "\r\n",
    "def attention_gate(X, g, channel,  \r\n",
    "                   activation='ReLU', \r\n",
    "                   attention='add', name='att'):\r\n",
    "    '''\r\n",
    "    Self-attention gate modified from Oktay et al. 2018.\r\n",
    "    \r\n",
    "    attention_gate(X, g, channel,  activation='ReLU', attention='add', name='att')\r\n",
    "    \r\n",
    "    Input\r\n",
    "    ----------\r\n",
    "        X: input tensor, i.e., key and value.\r\n",
    "        g: gated tensor, i.e., query.\r\n",
    "        channel: number of intermediate channel.\r\n",
    "                 Oktay et al. (2018) did not specify (denoted as F_int).\r\n",
    "                 intermediate channel is expected to be smaller than the input channel.\r\n",
    "        activation: a nonlinear attnetion activation.\r\n",
    "                    The `sigma_1` in Oktay et al. 2018. Default is 'ReLU'.\r\n",
    "        attention: 'add' for additive attention; 'multiply' for multiplicative attention.\r\n",
    "                   Oktay et al. 2018 applied additive attention.\r\n",
    "        name: prefix of the created keras layers.\r\n",
    "        \r\n",
    "    Output\r\n",
    "    ----------\r\n",
    "        X_att: output tensor.\r\n",
    "    \r\n",
    "    '''\r\n",
    "    activation_func = eval(activation)\r\n",
    "    attention_func = eval(attention)\r\n",
    "    \r\n",
    "    # mapping the input tensor to the intermediate channel\r\n",
    "    theta_att = Conv2D(channel, 1, use_bias=True, name='{}_theta_x'.format(name))(X)\r\n",
    "    \r\n",
    "    # mapping the gate tensor\r\n",
    "    phi_g = Conv2D(channel, 1, use_bias=True, name='{}_phi_g'.format(name))(g)\r\n",
    "    \r\n",
    "    # ----- attention learning ----- #\r\n",
    "    query = attention_func([theta_att, phi_g], name='{}_add'.format(name))\r\n",
    "    \r\n",
    "    # nonlinear activation\r\n",
    "    f = activation_func(name='{}_activation'.format(name))(query)\r\n",
    "    \r\n",
    "    # linear transformation\r\n",
    "    psi_f = Conv2D(1, 1, use_bias=True, name='{}_psi_f'.format(name))(f)\r\n",
    "    # ------------------------------ #\r\n",
    "    \r\n",
    "    # sigmoid activation as attention coefficients\r\n",
    "    coef_att = Activation('sigmoid', name='{}_sigmoid'.format(name))(psi_f)\r\n",
    "    \r\n",
    "    # multiplicative attention masking\r\n",
    "    X_att = multiply([X, coef_att], name='{}_masking'.format(name))\r\n",
    "    \r\n",
    "    return X_att\r\n",
    "\r\n",
    "def CONV_stack(X, channel, kernel_size=3, stack_num=2, \r\n",
    "               dilation_rate=1, activation='ReLU', \r\n",
    "               batch_norm=False, name='conv_stack'):\r\n",
    "    '''\r\n",
    "    Stacked convolutional layers:\r\n",
    "    (Convolutional layer --> batch normalization --> Activation)*stack_num\r\n",
    "    \r\n",
    "    CONV_stack(X, channel, kernel_size=3, stack_num=2, dilation_rate=1, activation='ReLU', \r\n",
    "               batch_norm=False, name='conv_stack')\r\n",
    "    \r\n",
    "    \r\n",
    "    Input\r\n",
    "    ----------\r\n",
    "        X: input tensor.\r\n",
    "        channel: number of convolution filters.\r\n",
    "        kernel_size: size of 2-d convolution kernels.\r\n",
    "        stack_num: number of stacked Conv2D-BN-Activation layers.\r\n",
    "        dilation_rate: optional dilated convolution kernel.\r\n",
    "        activation: one of the `tensorflow.keras.layers` interface, e.g., ReLU.\r\n",
    "        batch_norm: True for batch normalization, False otherwise.\r\n",
    "        name: prefix of the created keras layers.\r\n",
    "        \r\n",
    "    Output\r\n",
    "    ----------\r\n",
    "        X: output tensor\r\n",
    "        \r\n",
    "    '''\r\n",
    "    \r\n",
    "    bias_flag = not batch_norm\r\n",
    "    \r\n",
    "    # stacking Convolutional layers\r\n",
    "    for i in range(stack_num):\r\n",
    "        \r\n",
    "        activation_func = eval(activation)\r\n",
    "        \r\n",
    "        # linear convolution\r\n",
    "        X = Conv2D(channel, kernel_size, padding='same', use_bias=bias_flag, \r\n",
    "                   dilation_rate=dilation_rate, name='{}_{}'.format(name, i))(X)\r\n",
    "        \r\n",
    "        # batch normalization\r\n",
    "        if batch_norm:\r\n",
    "            X = BatchNormalization(axis=3, name='{}_{}_bn'.format(name, i))(X)\r\n",
    "        \r\n",
    "        # activation\r\n",
    "        activation_func = eval(activation)\r\n",
    "        X = activation_func(name='{}_{}_activation'.format(name, i))(X)\r\n",
    "        \r\n",
    "    return X\r\n",
    "\r\n",
    "def Res_CONV_stack(X, X_skip, channel, res_num, activation='ReLU', batch_norm=False, name='res_conv'):\r\n",
    "    '''\r\n",
    "    Stacked convolutional layers with residual path.\r\n",
    "     \r\n",
    "    Res_CONV_stack(X, X_skip, channel, res_num, activation='ReLU', batch_norm=False, name='res_conv')\r\n",
    "     \r\n",
    "    Input\r\n",
    "    ----------\r\n",
    "        X: input tensor.\r\n",
    "        X_skip: the tensor that does go into the residual path \r\n",
    "                can be a copy of X (e.g., the identity block of ResNet).\r\n",
    "        channel: number of convolution filters.\r\n",
    "        res_num: number of convolutional layers within the residual path.\r\n",
    "        activation: one of the `tensorflow.keras.layers` interface, e.g., 'ReLU'.\r\n",
    "        batch_norm: True for batch normalization, False otherwise.\r\n",
    "        name: prefix of the created keras layers.\r\n",
    "        \r\n",
    "    Output\r\n",
    "    ----------\r\n",
    "        X: output tensor.\r\n",
    "        \r\n",
    "    '''  \r\n",
    "    X = CONV_stack(X, channel, kernel_size=3, stack_num=res_num, dilation_rate=1, \r\n",
    "                   activation=activation, batch_norm=batch_norm, name=name)\r\n",
    "\r\n",
    "    X = add([X_skip, X], name='{}_add'.format(name))\r\n",
    "    \r\n",
    "    activation_func = eval(activation)\r\n",
    "    X = activation_func(name='{}_add_activation'.format(name))(X)\r\n",
    "    \r\n",
    "    return X\r\n",
    "\r\n",
    "def Sep_CONV_stack(X, channel, kernel_size=3, stack_num=1, dilation_rate=1, activation='ReLU', batch_norm=False, name='sep_conv'):\r\n",
    "    '''\r\n",
    "    Depthwise separable convolution with (optional) dilated convolution kernel and batch normalization.\r\n",
    "    \r\n",
    "    Sep_CONV_stack(X, channel, kernel_size=3, stack_num=1, dilation_rate=1, activation='ReLU', batch_norm=False, name='sep_conv')\r\n",
    "    \r\n",
    "    Input\r\n",
    "    ----------\r\n",
    "        X: input tensor.\r\n",
    "        channel: number of convolution filters.\r\n",
    "        kernel_size: size of 2-d convolution kernels.\r\n",
    "        stack_num: number of stacked depthwise-pointwise layers.\r\n",
    "        dilation_rate: optional dilated convolution kernel.\r\n",
    "        activation: one of the `tensorflow.keras.layers` interface, e.g., 'ReLU'.\r\n",
    "        batch_norm: True for batch normalization, False otherwise.\r\n",
    "        name: prefix of the created keras layers.\r\n",
    "        \r\n",
    "    Output\r\n",
    "    ----------\r\n",
    "        X: output tensor.\r\n",
    "    \r\n",
    "    '''\r\n",
    "    \r\n",
    "    activation_func = eval(activation)\r\n",
    "    bias_flag = not batch_norm\r\n",
    "    \r\n",
    "    for i in range(stack_num):\r\n",
    "        X = DepthwiseConv2D(kernel_size, dilation_rate=dilation_rate, padding='same', \r\n",
    "                            use_bias=bias_flag, name='{}_{}_depthwise'.format(name, i))(X)\r\n",
    "        \r\n",
    "        if batch_norm:\r\n",
    "            X = BatchNormalization(name='{}_{}_depthwise_BN'.format(name, i))(X)\r\n",
    "\r\n",
    "        X = activation_func(name='{}_{}_depthwise_activation'.format(name, i))(X)\r\n",
    "\r\n",
    "        X = Conv2D(channel, (1, 1), padding='same', use_bias=bias_flag, name='{}_{}_pointwise'.format(name, i))(X)\r\n",
    "        \r\n",
    "        if batch_norm:\r\n",
    "            X = BatchNormalization(name='{}_{}_pointwise_BN'.format(name, i))(X)\r\n",
    "\r\n",
    "        X = activation_func(name='{}_{}_pointwise_activation'.format(name, i))(X)\r\n",
    "    \r\n",
    "    return X\r\n",
    "\r\n",
    "def ASPP_conv(X, channel, activation='ReLU', batch_norm=True, name='aspp'):\r\n",
    "    '''\r\n",
    "    Atrous Spatial Pyramid Pooling (ASPP).\r\n",
    "    \r\n",
    "    ASPP_conv(X, channel, activation='ReLU', batch_norm=True, name='aspp')\r\n",
    "    \r\n",
    "    ----------\r\n",
    "    Wang, Y., Liang, B., Ding, M. and Li, J., 2019. Dense semantic labeling \r\n",
    "    with atrous spatial pyramid pooling and decoder for high-resolution remote \r\n",
    "    sensing imagery. Remote Sensing, 11(1), p.20.\r\n",
    "    \r\n",
    "    Input\r\n",
    "    ----------\r\n",
    "        X: input tensor.\r\n",
    "        channel: number of convolution filters.\r\n",
    "        activation: one of the `tensorflow.keras.layers` interface, e.g., ReLU.\r\n",
    "        batch_norm: True for batch normalization, False otherwise.\r\n",
    "        name: prefix of the created keras layers.\r\n",
    "        \r\n",
    "    Output\r\n",
    "    ----------\r\n",
    "        X: output tensor.\r\n",
    "        \r\n",
    "    * dilation rates are fixed to `[6, 9, 12]`.\r\n",
    "    '''\r\n",
    "    \r\n",
    "    activation_func = eval(activation)\r\n",
    "    bias_flag = not batch_norm\r\n",
    "\r\n",
    "    shape_before = X.get_shape().as_list()\r\n",
    "    b4 = GlobalAveragePooling2D(name='{}_avepool_b4'.format(name))(X)\r\n",
    "    \r\n",
    "    b4 = expand_dims(expand_dims(b4, 1), 1, name='{}_expdim_b4'.format(name))\r\n",
    "    \r\n",
    "    b4 = Conv2D(channel, 1, padding='same', use_bias=bias_flag, name='{}_conv_b4'.format(name))(b4)\r\n",
    "    \r\n",
    "    if batch_norm:\r\n",
    "        b4 = BatchNormalization(name='{}_conv_b4_BN'.format(name))(b4)\r\n",
    "        \r\n",
    "    b4 = activation_func(name='{}_conv_b4_activation'.format(name))(b4)\r\n",
    "    \r\n",
    "    # <----- tensorflow v1 resize.\r\n",
    "    b4 = Lambda(lambda X: image.resize(X, shape_before[1:3], method='bilinear', align_corners=True), \r\n",
    "                name='{}_resize_b4'.format(name))(b4)\r\n",
    "    \r\n",
    "    b0 = Conv2D(channel, (1, 1), padding='same', use_bias=bias_flag, name='{}_conv_b0'.format(name))(X)\r\n",
    "\r\n",
    "    if batch_norm:\r\n",
    "        b0 = BatchNormalization(name='{}_conv_b0_BN'.format(name))(b0)\r\n",
    "        \r\n",
    "    b0 = activation_func(name='{}_conv_b0_activation'.format(name))(b0)\r\n",
    "    \r\n",
    "    # dilation rates are fixed to `[6, 9, 12]`.\r\n",
    "    b_r6 = Sep_CONV_stack(X, channel, kernel_size=3, stack_num=1, activation='ReLU', \r\n",
    "                        dilation_rate=6, batch_norm=True, name='{}_sepconv_r6'.format(name))\r\n",
    "    b_r9 = Sep_CONV_stack(X, channel, kernel_size=3, stack_num=1, activation='ReLU', \r\n",
    "                        dilation_rate=9, batch_norm=True, name='{}_sepconv_r9'.format(name))\r\n",
    "    b_r12 = Sep_CONV_stack(X, channel, kernel_size=3, stack_num=1, activation='ReLU', \r\n",
    "                        dilation_rate=12, batch_norm=True, name='{}_sepconv_r12'.format(name))\r\n",
    "    \r\n",
    "    return concatenate([b4, b0, b_r6, b_r9, b_r12])\r\n",
    "\r\n",
    "def CONV_output(X, n_labels, kernel_size=1, activation='Softmax', name='conv_output'):\r\n",
    "    '''\r\n",
    "    Convolutional layer with output activation.\r\n",
    "    \r\n",
    "    CONV_output(X, n_labels, kernel_size=1, activation='Softmax', name='conv_output')\r\n",
    "    \r\n",
    "    Input\r\n",
    "    ----------\r\n",
    "        X: input tensor.\r\n",
    "        n_labels: number of classification label(s).\r\n",
    "        kernel_size: size of 2-d convolution kernels. Default is 1-by-1.\r\n",
    "        activation: one of the `tensorflow.keras.layers` or `keras_unet_collection.activations` interface or 'Sigmoid'.\r\n",
    "                    Default option is 'Softmax'.\r\n",
    "                    if None is received, then linear activation is applied.\r\n",
    "        name: prefix of the created keras layers.\r\n",
    "        \r\n",
    "    Output\r\n",
    "    ----------\r\n",
    "        X: output tensor.\r\n",
    "        \r\n",
    "    '''\r\n",
    "    \r\n",
    "    X = Conv2D(n_labels, kernel_size, padding='same', use_bias=True, name=name)(X)\r\n",
    "    \r\n",
    "    if activation:\r\n",
    "        \r\n",
    "        if activation == 'Sigmoid':\r\n",
    "            X = Activation('sigmoid', name='{}_activation'.format(name))(X)\r\n",
    "            \r\n",
    "        else:\r\n",
    "            activation_func = eval(activation)\r\n",
    "            X = activation_func(name='{}_activation'.format(name))(X)\r\n",
    "            \r\n",
    "    return X\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from tensorflow.keras.layers import Input\r\n",
    "from tensorflow.keras.models import Model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "c7118b647d8aee2f8c09effbe5401c08cfa1b0b54a052ea455fdde2a36cad5c3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}